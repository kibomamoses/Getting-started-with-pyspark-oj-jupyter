{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySfXD8RCG7G0"
   },
   "source": [
    "# Problem Definition\n",
    "#### How to determine the *price* of a used car?\n",
    "by Moses kiboma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap3hBFz5G7G4"
   },
   "source": [
    "## Contents\n",
    "\n",
    "[Installation Setup](#Installation-Setup) <br>\n",
    "+   [Environment Config](#Environment-Configuration) <br>\n",
    "+   [Python Packages](#Loading-Packages) <br>\n",
    "+   [Apache Spark](#Creating-SparkSession) <br>\n",
    "\n",
    "[Extract, Transform, Load](#Extract-Stage) <br>\n",
    "This includes the various stages of the ETL Pipeline <br>\n",
    "+   [Extract](#Extract-Stage) <br>\n",
    "    +   [TrueCar Used Car Listed Data](#Truecar.com) <br>\n",
    "    +   [Validating Data](#Validating-Data) <br>\n",
    "    +   [Cleaning Data (Basic)](#Cleaning-Data-(Basic)) <br>\n",
    "    +   [Caching Data on S3](#Caching-Extract-Data-on-S3) <br>\n",
    "+   [Transform](#Transform-Stage) <br>\n",
    "    +   [Cleaning Data](#Cleaning-Data) <br>\n",
    "    +   [Feature Engineering](#Feature-Engineering) <br>\n",
    "    +   [Sampling Data](#Sampling-Data) <br>\n",
    "    +   [Exploratory Data Analysis using Pandas, Matplotlib and Seaborn](#Exploratory-Data-Analysis) <br>\n",
    "    +   [Caching Data on S3](#Caching-Transform-Data-on-S3) <br>\n",
    "+   [Load](#Load-Stage) <br>\n",
    "    +   [Preprocessing Data for Learning Model](#Load-Data)\n",
    "    +   [Migrate Data to Database](#Load-Data)\n",
    "    \n",
    "[Predicting Used Car Price](#Machine-Learning)\n",
    "+   Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkPRE7ecG7G5"
   },
   "source": [
    "# Installation Setup\n",
    "\n",
    "## Tool Versions\n",
    "\n",
    "```\n",
    "Apache Spark - 2.4.3\n",
    "Jupyter Notebook - 4.4.0\n",
    "```\n",
    "    \n",
    "## Environment Configuration\n",
    "\n",
    "#### Configuring ~/.bash_profile\n",
    "\n",
    "```\n",
    "export PATH=\"/usr/local/bin:$PATH\"\n",
    "PATH=\"/Library/Frameworks/Python.framework/Versions/3.7/bin:${PATH}\"\n",
    "export PATH=/usr/local/scala/bin:$PATH\n",
    "export PATH=/usr/local/spark/bin:$PATH\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\n",
    "export PYSPARK_PYTHON=python3.7\n",
    "```\n",
    "\n",
    "#### Configuring ~/.bashrc\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=/usr/local/bin/python3.7\n",
    "export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3.7\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrwNp4JYWG7L",
    "outputId": "a9b1e42e-5fb0-48ba-e208-7af793d1e2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a1AKvjeG7G7"
   },
   "source": [
    "### Findspark\n",
    "\n",
    "Use `findspark` to be able to find and import **Pyspark** module, while correctly setting environmental variables and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADdDWJ36G7G8",
    "outputId": "bd446385-cac2-437c-fc2d-67ca0835c080"
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import findspark\n",
    "try:\n",
    "    findspark.init('/opt/spark')\n",
    "except:\n",
    "    print (\"Error:\", ''.join(traceback.format_stack()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbHCJ-f0G7G-"
   },
   "source": [
    "Check paths before Executing PySpark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "f_itQLOXG7G_",
    "outputId": "cf5cc0c8-780a-4c69-dfb0-dd83ff68f09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: /home/moses/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "\n",
      "SPARK_HOME: /opt/spark\n",
      "PYSPARK_PYTHON: /bin/python3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(\"PATH: %s\\n\" % os.environ['PATH'])\n",
    "print(\"SPARK_HOME: %s\" % os.environ['SPARK_HOME'])\n",
    "print(\"PYSPARK_PYTHON: %s\" % os.environ['PYSPARK_PYTHON'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xM8Oz_VG7HC"
   },
   "source": [
    "## Loading Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zm8g8_weG7HD"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MxTqYZsG7HE"
   },
   "source": [
    "### Package Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fFysMncxG7HF",
    "outputId": "786941b9-3899-4cbe-c121-8e775f7e8f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.4.1\n",
      "numpy: 1.22.2\n",
      "matplotlib: 3.5.1\n",
      "seaborn: 0.11.2\n",
      "Python: 3.8.10 (default, Nov 26 2021, 20:14:08) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y12pSNYvG7HG"
   },
   "source": [
    "## Creating SparkSession\n",
    "Get package to handle AWS to access S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SE3ZbsOIG7HH",
    "outputId": "ab4e826c-6d76-436a-d78a-b331bcb7d201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_SUBMIT_ARGS=--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "%set_env PYSPARK_SUBMIT_ARGS=--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbc0sMxWG7HI"
   },
   "source": [
    "Creating Spark Session, hosted across all local nodes on a **Standalone Cluster**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yktx41bLG7HJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/23 17:58:26 WARN Utils: Your hostname, kiboma resolves to a loopback address: 127.0.1.1; using 192.168.0.107 instead (on interface wlo1)\n",
      "22/03/23 17:58:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/moses/.ivy2/cache\n",
      "The jars for the packages stored in: /home/moses/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ec1fa297-91fd-4097-946b-9ba461ceb0e2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.3 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.3 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.10.14 in central\n",
      "\t[2.10.14] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 13402ms :: artifacts dl 202ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.14 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.3 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ec1fa297-91fd-4097-946b-9ba461ceb0e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/41ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/23 17:58:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PySpark Craigslist\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AvrMVgWG7HJ"
   },
   "source": [
    "Configure Hadoop connection for S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M62QYhCcG7HK"
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", \"AKIAQQADETG4GH4VNP\")\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", \"wXni64+7npCqxCDB14zMlyf4D6D2LR+B/M1pvz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnMXwJpBG7HL"
   },
   "source": [
    "Monitoring Spark instrumentation through the WebUI available through `localhost:4040/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_peTFrzG7HL"
   },
   "source": [
    "# Extract Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya984W75G7HM"
   },
   "source": [
    "## Project Dataset\n",
    "\n",
    "Available [https://www.truecar.com/used-cars-for-sale/listings/](https://www.truecar.com/used-cars-for-sale/listings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_NkajG-iG7HN",
    "outputId": "99d092bb-0331-4a17-8cac-066b0939855a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicle_listings = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"clean_car_dataset.csv\")\n",
    "type(vehicle_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIFNa-DrG7HN"
   },
   "source": [
    "## Validating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW9Fag79G7HO"
   },
   "source": [
    "Now that the data is available as a local *dataframe* on the Spark cluster, let's validate the dataframe by look at the schema, size, samples and statistics of our working data - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9PpXrm65G7HP",
    "outputId": "5dde4639-6512-481c-f75c-377f721598a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- PRICE: string (nullable = true)\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- MILEAGE: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- MAKE: string (nullable = true)\n",
      " |-- MODEL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle_listings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os-HrEIJG7HP"
   },
   "source": [
    "Dimensions of Raw Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GYewvvKhG7HQ",
    "outputId": "40f72542-9796-4178-d3bb-ef7b3c4589ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539107 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(vehicle_listings.count(),len(vehicle_listings.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA0nwm8XG7HQ"
   },
   "source": [
    "Collecting random sample to see what kind of data populates each column:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PySpark Tasks Used car vehicles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
